{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Tweets.csv') as csv_file:\n",
    "    read_CSV = csv.reader(csv_file, delimiter=',')\n",
    "    # sentiment col # 1\n",
    "    Y = list()\n",
    "    # reviews col # 10\n",
    "    X_text = list()\n",
    "    for row in read_CSV:\n",
    "        Y.append(row[1])\n",
    "        X_text.append(row[10])\n",
    "    Y = Y[1:]\n",
    "    X_text = X_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral @VirginAmerica What @dhepburn said. 1.0 Virgin America\n"
     ]
    }
   ],
   "source": [
    "print(Y[0],X_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "X_text_new = list()\n",
    "Y_new = list()\n",
    "sentiment_confidence_new = list()\n",
    "airlines_new = list()\n",
    "\n",
    "for idx, tweet in enumerate(X_text.copy()):\n",
    "    if 'RT' in tweet:\n",
    "        continue\n",
    "    \n",
    "    if len(tweet) < 20:\n",
    "        continue\n",
    "        \n",
    "    if detect(tweet) != 'en':\n",
    "        continue\n",
    "        \n",
    "    X_text_new.append(X_text[idx])\n",
    "    Y_new.append(Y[idx])\n",
    "    \n",
    "X_text = X_text_new\n",
    "Y = Y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "# remove urls and strip\n",
    "X_text = [re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text).strip() for text in X_text]\n",
    "\n",
    "# remove punctuation\n",
    "punctuation += \"â€œâ€™â€¦â€\"\n",
    "X_text = [''.join([char for char in text if char not in punctuation]) for text in X_text]\n",
    "\n",
    "# tokenize tweets\n",
    "X_tokenized = [word_tokenize(text) for text in X_text]\n",
    "\n",
    "# stemming\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "X_tokenized = [[snowball_stemmer.stem(word) for word in words] for words in X_tokenized]\n",
    "\n",
    "# remove stopwords\n",
    "X_tokenized = [[word for word in words if word not in stopwords.words('english')] for words in X_tokenized]\n",
    "\n",
    "X = [' '.join(words) for words in X_tokenized]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virginamerica dhepburn said', 'virginamerica plus youv ad commerci experi tacki', 'virginamerica didnt today must mean need take anoth trip', 'virginamerica realli aggress blast obnoxi entertain guest face amp littl recours', 'virginamerica realli big bad thing', 'virginamerica serious would pay 30 flight seat didnt play realli onli bad thing fli va', 'virginamerica yes near everi time fli vx ear worm wont go away', 'virginamerica realli miss prime opportun men without hat parodi', 'virginamerica amaz arriv hour earli good', 'virginamerica know suicid second lead caus death among teen 1024', 'virginamerica lt3 pretti graphic much better minim iconographi', 'virginamerica great deal alreadi think 2nd trip australia amp havent even gone 1st trip yet p', 'virginamerica virginmedia im fli fabul seduct sky u take stress away travel', 'virginamerica thank', 'virginamerica sfopdx schedul still mia', 'virginamerica excit first cross countri flight lax mco ive heard noth great thing virgin america 29daystogo', 'virginamerica flew nyc sfo last week couldnt fulli sit seat due two larg gentleman either side help', 'â¤ï¸ fli virginamerica â˜ºï¸ðŸ‘', 'virginamerica know would amaz awesom bosfll pleas want fli onli', 'virginamerica whi first fare may three time carrier seat avail select']\n"
     ]
    }
   ],
   "source": [
    "print(X[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calculating the tf-idf scores\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11041, 10703)\n",
      "(2761, 10703)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_transformed.shape)\n",
    "print(X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the classifiers\n",
    "The three classifiers to be used (import them from sklearn) are:\n",
    "\n",
    "a) Multinomial Naive Bayes\n",
    "\n",
    "b) K Nearest Neighbors Classifier\n",
    "\n",
    "c) Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize classifier\n",
    "naive_bayes_clf = MultinomialNB()\n",
    "\n",
    "# Train classifier\n",
    "naive_bayes_clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6812749003984063\n"
     ]
    }
   ],
   "source": [
    "print(naive_bayes_clf.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6812749003984063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = naive_bayes_clf.predict(X_test_transformed)\n",
    "\n",
    "score = f1_score(y_test, y_pred, average='micro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize classifier\n",
    "k_nearest_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train classifier\n",
    "k_nearest_clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5896414342629482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = k_nearest_clf.predict(X_test_transformed)\n",
    "\n",
    "score = f1_score(y_test, y_pred, average='micro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize classifier\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "\n",
    "# Train classifier\n",
    "random_forest_clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7540746106483158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = random_forest_clf.predict(X_test_transformed)\n",
    "\n",
    "score = f1_score(y_test, y_pred, average='micro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work on the larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('train.csv', encoding=\"ISO-8859-1\") as csv_file:\n",
    "    read_CSV = csv.reader(csv_file, delimiter=',')\n",
    "    # sentiment col # 1\n",
    "    Y = list()\n",
    "    # reviews col # 10\n",
    "    X_text = list()\n",
    "    for row in read_CSV:\n",
    "        Y.append(row[0])\n",
    "        X_text.append(row[5])\n",
    "    Y = Y[1:]\n",
    "    X_text = X_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing urls..\n",
      "done removing punctuation..\n",
      "done tokenizing..\n",
      "done stemming..\n",
      "done removing stop words..\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "# remove urls and strip\n",
    "X_text = [re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text).strip() for text in X_text]\n",
    "print(\"Done removing urls..\")\n",
    "\n",
    "# remove punctuation\n",
    "punctuation += \"â€œâ€™â€¦â€\"\n",
    "X_text = [''.join([char for char in text if char not in punctuation]) for text in X_text]\n",
    "print(\"done removing punctuation..\")\n",
    "\n",
    "# tokenize tweets\n",
    "X_tokenized = [word_tokenize(text) for text in X_text]\n",
    "print(\"done tokenizing..\")\n",
    "\n",
    "# stemming\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "X_tokenized = [[snowball_stemmer.stem(word) for word in words] for words in X_tokenized]\n",
    "print(\"done stemming..\")\n",
    "\n",
    "# remove stopwords\n",
    "X_tokenized = [[word for word in words if word not in stopwords.words('english')] for words in X_tokenized]\n",
    "\n",
    "X = [' '.join(words) for words in X_tokenized]\n",
    "print(\"done removing stop words..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calculating the tf-idf scores\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize classifier\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "\n",
    "# Train classifier\n",
    "random_forest_clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749596875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = random_forest_clf.predict(X_test_transformed)\n",
    "\n",
    "score = f1_score(y_test, y_pred, average='micro')\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
